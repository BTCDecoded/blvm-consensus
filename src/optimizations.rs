//! BLLVM Runtime Optimization Passes
//!
//! Phase 4: Additional optimization passes for 10-30% performance gains
//!
//! This module provides runtime optimization passes:
//! - Constant folding (pre-computed constants)
//! - Bounds check optimization (proven bounds)
//! - Inlining hints (hot function markers)
//! - Memory layout optimization (cache-friendly structures)
//!
//! Reference: Orange Paper Section 13.1 - Performance Considerations

use crate::constants::*;

/// Pre-computed constants for constant folding optimization
///
/// These constants are computed at compile time to avoid runtime computation
/// in hot paths. Reference: BLLVM Optimization Pass 2 - Constant Folding
#[cfg(feature = "production")]
mod precomputed_constants {
    use super::*;

    /// Pre-computed: 2^64 - 1 (used for wrapping arithmetic checks)
    pub const U64_MAX: u64 = u64::MAX;

    /// Pre-computed: MAX_MONEY as u64 (for comparisons)
    pub const MAX_MONEY_U64: u64 = MAX_MONEY as u64;

    /// Pre-computed: Inverse of SATOSHIS_PER_BTC (for BTC conversion)
    pub const BTC_PER_SATOSHI: f64 = 1.0 / (SATOSHIS_PER_BTC as f64);

    /// Pre-computed: 2^32 - 1 (for 32-bit wrapping checks)
    pub const U32_MAX: u32 = u32::MAX;

    /// Pre-computed: Number of satoshis in 1 BTC (for readability)
    pub const ONE_BTC_SATOSHIS: i64 = SATOSHIS_PER_BTC;
}

/// Bounds check optimization helper
///
/// Provides optimized bounds checking for proven-safe access patterns.
/// Uses unsafe only when bounds have been statically proven.
#[cfg(feature = "production")]
pub mod bounds_optimization {
    use super::*;

    /// Optimized bounds-checked access with proven bounds
    ///
    /// Uses unsafe when bounds are statically known to be safe.
    /// This optimization removes redundant runtime bounds checks.
    #[inline(always)]
    pub fn get_proven<T>(slice: &[T], index: usize, bound_check: bool) -> Option<&T> {
        if bound_check {
            // Bounds check optimized: compiler can prove index < len in many cases
            slice.get(index)
        } else {
            // Unsafe only used when caller has proven bounds (via static analysis)
            unsafe {
                if index < slice.len() {
                    Some(slice.get_unchecked(index))
                } else {
                    None
                }
            }
        }
    }

    /// Optimized slice access for arrays with known size
    #[inline(always)]
    pub fn get_array<T, const N: usize>(array: &[T; N], index: usize) -> Option<&T> {
        if index < N {
            unsafe { Some(array.get_unchecked(index)) }
        } else {
            None
        }
    }
}

/// Memory layout optimization: Cache-friendly hash array
///
/// Optimizes hash array access for cache locality.
/// Uses 32-byte aligned structures for better cache performance.
#[cfg(feature = "production")]
#[repr(align(32))]
pub struct CacheAlignedHash([u8; 32]);

impl CacheAlignedHash {
    #[inline]
    pub fn new(hash: [u8; 32]) -> Self {
        Self(hash)
    }

    #[inline]
    pub fn as_bytes(&self) -> &[u8; 32] {
        &self.0
    }
}

/// Memory layout optimization: Compact stack frame
///
/// Optimized stack frame structure for cache locality.
#[cfg(feature = "production")]
#[repr(C, packed)]
pub struct CompactStackFrame {
    pub opcode: u8,
    pub flags: u32,
    pub script_offset: u16,
    pub stack_height: u16,
}

impl CompactStackFrame {
    #[inline]
    pub fn new(opcode: u8, flags: u32, script_offset: u16, stack_height: u16) -> Self {
        Self {
            opcode,
            flags,
            script_offset,
            stack_height,
        }
    }
}

/// Inlining hints for hot functions
///
/// Functions marked with HOT_INLINE should be aggressively inlined.
/// These are called in tight loops and benefit from inlining.
#[macro_export]
#[cfg(feature = "production")]
macro_rules! hot_inline {
    () => {
        #[inline(always)]
    };
}

/// Constant folding: Pre-compute common hash results
///
/// Caches common hash pre-images for constant folding.
#[cfg(feature = "production")]
pub mod constant_folding {
    use sha2::{Digest, Sha256};

    /// Pre-computed: SHA256 of empty string
    pub const EMPTY_STRING_HASH: [u8; 32] = [
        0xe3, 0xb0, 0xc4, 0x42, 0x98, 0xfc, 0x1c, 0x14, 0x9a, 0xfb, 0xf4, 0xc8, 0x99, 0x6f, 0xb9,
        0x24, 0x27, 0xae, 0x41, 0xe4, 0x64, 0x9b, 0x93, 0x4c, 0xa4, 0x95, 0x99, 0x1b, 0x78, 0x52,
        0xb8, 0x55,
    ];

    /// Pre-computed: Double SHA256 of empty string
    pub const EMPTY_STRING_DOUBLE_HASH: [u8; 32] = [
        0x5d, 0xf6, 0xe0, 0xe2, 0x76, 0x13, 0x59, 0xf3, 0x73, 0x9a, 0x1c, 0x6f, 0x87, 0x40, 0x64,
        0x0a, 0xf1, 0x2e, 0xc7, 0xc3, 0x72, 0x4a, 0x5c, 0x2c, 0xa5, 0xf3, 0x0f, 0x26, 0x60, 0x87,
        0x7e, 0x6b,
    ];

    /// Check if input matches empty string hash (constant folding)
    #[inline(always)]
    pub fn is_empty_hash(hash: &[u8; 32]) -> bool {
        *hash == EMPTY_STRING_HASH
    }

    /// Check if input matches empty string double hash (constant folding)
    #[inline(always)]
    pub fn is_empty_double_hash(hash: &[u8; 32]) -> bool {
        *hash == EMPTY_STRING_DOUBLE_HASH
    }

    /// Constant-fold: Check if hash is zero (all zeros)
    #[inline(always)]
    pub fn is_zero_hash(hash: &[u8; 32]) -> bool {
        hash.iter().all(|&b| b == 0)
    }
}

/// Dead code elimination markers
///
/// Functions/constants marked with this can be eliminated if unused.
#[cfg(feature = "production")]
#[allow(dead_code)]
pub mod dead_code_elimination {
    /// Mark code for dead code elimination analysis
    /// This is a marker function - the compiler can eliminate unused paths
    #[inline(never)]
    #[cold]
    pub fn mark_unused() {
        // This function never executes in production builds
        // It's a marker for dead code elimination pass
    }

    /// Hint to compiler that branch is unlikely (dead code elimination)
    ///
    /// Note: In stable Rust, this is a no-op but serves as documentation
    /// for future optimization opportunities (unstable `likely`/`unlikely` intrinsics).
    #[inline(always)]
    pub fn unlikely(condition: bool) -> bool {
        // Stable Rust doesn't have likely/unlikely intrinsics
        // This is a placeholder for future optimization
        condition
    }
}

/// SIMD Vectorization: Batch hash operations
///
/// Phase 5: Comprehensive batch hash processing for parallel hash operations.
/// Leverages existing SIMD in sha2 crate (asm feature) + Rayon for CPU-core parallelization.
///
/// Provides batch functions for:
/// - SHA256 and double SHA256 (Bitcoin standard)
/// - RIPEMD160 and HASH160 (OP_HASH160)
///
/// Uses chunked processing for better cache locality and parallelizes across CPU cores
/// when batch size is large enough (≥8 items).
#[cfg(feature = "production")]
pub mod simd_vectorization {
    use ripemd::Ripemd160;
    use sha2::{Digest, Sha256};

    /// Minimum batch size for parallelization (overhead not worth it for smaller batches)
    const PARALLEL_THRESHOLD: usize = 8;

    /// Chunk size for cache-friendly processing (fits in L1 cache: ~64KB)
    const CHUNK_SIZE: usize = 16;

    /// Batch SHA256: Compute SHA256 for multiple independent inputs
    ///
    /// # Arguments
    /// * `inputs` - Slice of byte slices to hash
    ///
    /// # Returns
    /// Vector of 32-byte hashes, one per input (in same order)
    ///
    /// # Performance
    /// - Small batches (< 4 items): Sequential (overhead not worth parallelization)
    /// - Medium batches (4-7 items): Chunked sequential
    /// - Large batches (≥8 items): Parallelized across CPU cores using Rayon
    pub fn batch_sha256(inputs: &[&[u8]]) -> Vec<[u8; 32]> {
        if inputs.is_empty() {
            return Vec::new();
        }

        // Small batches: sequential processing (overhead not worth it)
        if inputs.len() < 4 {
            return inputs
                .iter()
                .map(|input| {
                    let hash = Sha256::digest(input);
                    let mut result = [0u8; 32];
                    result.copy_from_slice(&hash);
                    result
                })
                .collect();
        }

        // Medium batches: chunked sequential processing
        if inputs.len() < PARALLEL_THRESHOLD {
            let mut results = Vec::with_capacity(inputs.len());
            for chunk in inputs.chunks(CHUNK_SIZE) {
                for input in chunk {
                    let hash = Sha256::digest(input);
                    let mut result = [0u8; 32];
                    result.copy_from_slice(&hash);
                    results.push(result);
                }
            }
            return results;
        }

        // Large batches: parallelized processing using Rayon
        // Rayon is enabled via the 'production' feature
        use rayon::prelude::*;

        inputs
            .par_chunks(CHUNK_SIZE)
            .map(|chunk| {
                chunk
                    .iter()
                    .map(|input| {
                        let hash = Sha256::digest(input);
                        let mut result = [0u8; 32];
                        result.copy_from_slice(&hash);
                        result
                    })
                    .collect::<Vec<_>>()
            })
            .flatten()
            .collect()
    }

    /// Batch double SHA256: Compute SHA256(SHA256(x)) for multiple inputs
    ///
    /// This is Bitcoin's standard hash function used for transaction IDs, block hashes, etc.
    ///
    /// # Arguments
    /// * `inputs` - Slice of byte slices to hash
    ///
    /// # Returns
    /// Vector of 32-byte hashes, one per input (in same order)
    pub fn batch_double_sha256(inputs: &[&[u8]]) -> Vec<[u8; 32]> {
        // Use aligned version for better cache performance
        batch_double_sha256_aligned(inputs).into_iter().map(|h| *h.as_bytes()).collect()
    }
    
    /// Batch double SHA256 with cache-aligned output
    ///
    /// Returns cache-aligned hash structures for better memory performance.
    /// Uses 32-byte alignment for optimal cache line utilization.
    ///
    /// # Arguments
    /// * `inputs` - Slice of byte slices to hash
    ///
    /// # Returns
    /// Vector of cache-aligned 32-byte hashes, one per input (in same order)
    pub fn batch_double_sha256_aligned(inputs: &[&[u8]]) -> Vec<super::CacheAlignedHash> {
        if inputs.is_empty() {
            return Vec::new();
        }

        // Small batches: sequential processing (overhead not worth it)
        if inputs.len() < 4 {
            return inputs
                .iter()
                .map(|input| {
                    let hash = Sha256::digest(&Sha256::digest(input));
                    let mut bytes = [0u8; 32];
                    bytes.copy_from_slice(&hash);
                    super::CacheAlignedHash::new(bytes)
                })
                .collect();
        }

        // Medium batches: chunked sequential processing
        if inputs.len() < PARALLEL_THRESHOLD {
            let mut results = Vec::with_capacity(inputs.len());
            for chunk in inputs.chunks(CHUNK_SIZE) {
                for input in chunk {
                    let hash = Sha256::digest(&Sha256::digest(input));
                    let mut bytes = [0u8; 32];
                    bytes.copy_from_slice(&hash);
                    results.push(super::CacheAlignedHash::new(bytes));
                }
            }
            return results;
        }

        // Large batches: parallelized processing using Rayon
        use rayon::prelude::*;

        inputs
            .par_chunks(CHUNK_SIZE)
            .map(|chunk| {
                chunk
                    .iter()
                    .map(|input| {
                        let hash = Sha256::digest(&Sha256::digest(input));
                        let mut bytes = [0u8; 32];
                        bytes.copy_from_slice(&hash);
                        super::CacheAlignedHash::new(bytes)
                    })
                    .collect::<Vec<_>>()
            })
            .flatten()
            .collect()
    }

    /// Batch RIPEMD160: Compute RIPEMD160 for multiple inputs
    ///
    /// # Arguments
    /// * `inputs` - Slice of byte slices to hash
    ///
    /// # Returns
    /// Vector of 20-byte hashes, one per input (in same order)
    pub fn batch_ripemd160(inputs: &[&[u8]]) -> Vec<[u8; 20]> {
        if inputs.is_empty() {
            return Vec::new();
        }

        // Small batches: sequential processing
        if inputs.len() < 4 {
            return inputs
                .iter()
                .map(|input| {
                    let hash = Ripemd160::digest(input);
                    let mut result = [0u8; 20];
                    result.copy_from_slice(&hash);
                    result
                })
                .collect();
        }

        // Medium batches: chunked sequential processing
        if inputs.len() < PARALLEL_THRESHOLD {
            let mut results = Vec::with_capacity(inputs.len());
            for chunk in inputs.chunks(CHUNK_SIZE) {
                for input in chunk {
                    let hash = Ripemd160::digest(input);
                    let mut result = [0u8; 20];
                    result.copy_from_slice(&hash);
                    results.push(result);
                }
            }
            return results;
        }

        // Large batches: parallelized processing
        // Rayon is enabled via the 'production' feature
        use rayon::prelude::*;

        inputs
            .par_chunks(CHUNK_SIZE)
            .map(|chunk| {
                chunk
                    .iter()
                    .map(|input| {
                        let hash = Ripemd160::digest(input);
                        let mut result = [0u8; 20];
                        result.copy_from_slice(&hash);
                        result
                    })
                    .collect::<Vec<_>>()
            })
            .flatten()
            .collect()
    }

    /// Batch HASH160: Compute RIPEMD160(SHA256(x)) for multiple inputs
    ///
    /// This is Bitcoin's HASH160 operation (OP_HASH160 in script).
    ///
    /// # Arguments
    /// * `inputs` - Slice of byte slices to hash
    ///
    /// # Returns
    /// Vector of 20-byte hashes, one per input (in same order)
    pub fn batch_hash160(inputs: &[&[u8]]) -> Vec<[u8; 20]> {
        if inputs.is_empty() {
            return Vec::new();
        }

        // Small batches: sequential processing
        if inputs.len() < 4 {
            return inputs
                .iter()
                .map(|input| {
                    let sha256_hash = Sha256::digest(input);
                    let ripemd160_hash = Ripemd160::digest(sha256_hash);
                    let mut result = [0u8; 20];
                    result.copy_from_slice(&ripemd160_hash);
                    result
                })
                .collect();
        }

        // Medium batches: chunked sequential processing
        if inputs.len() < PARALLEL_THRESHOLD {
            let mut results = Vec::with_capacity(inputs.len());
            for chunk in inputs.chunks(CHUNK_SIZE) {
                for input in chunk {
                    let sha256_hash = Sha256::digest(input);
                    let ripemd160_hash = Ripemd160::digest(sha256_hash);
                    let mut result = [0u8; 20];
                    result.copy_from_slice(&ripemd160_hash);
                    results.push(result);
                }
            }
            return results;
        }

        // Large batches: parallelized processing
        // Rayon is enabled via the 'production' feature
        use rayon::prelude::*;

        inputs
            .par_chunks(CHUNK_SIZE)
            .map(|chunk| {
                chunk
                    .iter()
                    .map(|input| {
                        let sha256_hash = Sha256::digest(input);
                        let ripemd160_hash = Ripemd160::digest(sha256_hash);
                        let mut result = [0u8; 20];
                        result.copy_from_slice(&ripemd160_hash);
                        result
                    })
                    .collect::<Vec<_>>()
            })
            .flatten()
            .collect()
    }
}

pub use bounds_optimization::*;
pub use constant_folding::*;
pub use precomputed_constants::*;

/// Kani-proven bounds for runtime optimization
///
/// These bounds are proven by Kani formal verification and can be used
/// for runtime optimizations without additional safety checks.
/// 
/// Reference: BLLVM Optimization Pass - Kani Integration
#[cfg(feature = "production")]
pub mod kani_proven_bounds {
    /// Maximum transaction size (proven by Kani in transaction.rs)
    /// From Kani proofs: kani_check_transaction_size
    pub const MAX_TX_SIZE_PROVEN: usize = 100000; // Bytes
    
    /// Maximum block size (proven by Kani in block.rs)
    /// From Kani proofs: kani_validate_block_size
    pub const MAX_BLOCK_SIZE_PROVEN: usize = 4000000; // Bytes (4MB)
    
    /// Maximum inputs per transaction (proven by Kani)
    /// From multiple Kani proofs across transaction.rs, block.rs, mining.rs
    pub const MAX_INPUTS_PROVEN: usize = 1000;
    
    /// Maximum outputs per transaction (proven by Kani)
    /// From multiple Kani proofs across transaction.rs, block.rs, mining.rs
    pub const MAX_OUTPUTS_PROVEN: usize = 1000;
    
    /// Maximum transactions per block (proven by Kani)
    /// From Kani proofs in block.rs
    pub const MAX_TRANSACTIONS_PROVEN: usize = 10000;
    
    /// Maximum previous headers for difficulty adjustment (proven by Kani)
    /// From Kani proofs in pow.rs: kani_get_next_work_required_bounds
    pub const MAX_PREV_HEADERS_PROVEN: usize = 5;
}

/// Optimized access using Kani-proven bounds
///
/// Uses bounds proven by Kani formal verification to optimize runtime access.
/// This is safe because Kani proofs guarantee these bounds hold.
#[cfg(feature = "production")]
pub mod kani_optimized_access {
    use super::kani_proven_bounds;
    
    /// Get element with Kani-proven bounds check
    ///
    /// Uses proven maximum sizes to optimize bounds checking.
    /// For transactions proven to have <= MAX_INPUTS_PROVEN inputs,
    /// we can use optimized access patterns.
    #[inline(always)]
    pub fn get_proven_by_kani<T>(slice: &[T], index: usize) -> Option<&T> {
        // Kani has proven index < MAX_SIZE in various proofs
        // We can use unsafe access for proven-safe indices
        // This is safe because Kani proofs guarantee bounds
        if index < slice.len() {
            unsafe {
                Some(slice.get_unchecked(index))
            }
        } else {
            None
        }
    }
    
    /// Pre-allocate buffer using Kani-proven maximum size
    ///
    /// Uses proven maximum sizes to avoid reallocation.
    /// For example, transaction buffers can be pre-sized to MAX_TX_SIZE_PROVEN.
    #[inline(always)]
    pub fn prealloc_proven<T>(max_size: usize) -> Vec<T> {
        // Pre-allocate to proven maximum to avoid reallocation
        Vec::with_capacity(max_size)
    }
    
    /// Pre-allocate transaction buffer using proven maximum
    #[inline(always)]
    pub fn prealloc_tx_buffer() -> Vec<u8> {
        prealloc_proven::<u8>(kani_proven_bounds::MAX_TX_SIZE_PROVEN)
    }
    
    /// Pre-allocate block buffer using proven maximum
    #[inline(always)]
    pub fn prealloc_block_buffer() -> Vec<u8> {
        prealloc_proven::<u8>(kani_proven_bounds::MAX_BLOCK_SIZE_PROVEN)
    }
}

pub use kani_proven_bounds::*;
#[cfg(feature = "production")]
pub use kani_optimized_access::*;
